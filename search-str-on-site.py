import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

visited = set()
matches = []

def crawl(url, domain):
    if url in visited:
        return
    visited.add(url)
    
    try:
        response = requests.get(url, timeout=5)
        if 'text/html' not in response.headers.get('Content-Type', ''):
            return
        
        html = response.text
        if re.search(r'\b(–§–µ–Ω–∏–∫—Å|—Ñ–µ–Ω—è–∫—Å)\b', html, re.IGNORECASE):
            print(f'üîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}')
            matches.append(url)
        
        soup = BeautifulSoup(html, 'html.parser')
        for link in soup.find_all('a', href=True):
            next_url = urljoin(url, link['href'])
            if urlparse(next_url).netloc == domain:
                crawl(next_url, domain)
    
    except Exception as e:
        print(f'‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}')

# –£–∫–∞–∑–∞—Ç—å –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
start_url = 'https://czm-fond.ru/'  # ‚Üê –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Å–∞–π—Ç
domain = urlparse(start_url).netloc

crawl(start_url, domain)

print('\n‚úÖ –ù–∞–π–¥–µ–Ω—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö:')
for match in matches:
    print(match)
